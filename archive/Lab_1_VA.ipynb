{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1cc9ae",
   "metadata": {},
   "source": [
    "# Visual attention - Lab 1 - From the eye-tracker to saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d2dd3",
   "metadata": {},
   "source": [
    "Created by : Alexandre Bruckert / University of Nantes - alexandre.bruckert@univ-nantes.fr\n",
    "\n",
    "Date : 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72daebf",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to process eye-tracking data as output by the eye-tracker in order to transform it into a fixation map, or a visual saliency map.\n",
    "\n",
    "To this end, you will have to fill out the functions specified in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "PATH_DATA = \"path\\to\\the\\dataset\\directory\"\n",
    "\n",
    "# Experiment-dependant constants\n",
    "RESO_X = 1280\n",
    "RESO_Y = 1024\n",
    "FACTOR_X = RESO_X/338\n",
    "FACTOR_Y = RESO_Y/270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824415d6",
   "metadata": {},
   "source": [
    "First, for each image, we want to gather the files from all observers output by the eye-tracker.\n",
    "For this dataset, file structure is the following :  \n",
    "IIRCCyN_IVC_Eyetracker_Berkeley_Database  \n",
    "> Images  \n",
    "> Eyetracker_Data  \n",
    ">>Obs1  \n",
    ">>.  \n",
    ">>.     \n",
    ">>ObsN  \n",
    ">>> image_img.trn.1.png-obs_ObsN.txt  \n",
    ">>>.  \n",
    ">>>.  \n",
    ">>> image_img.trn.N.png-obs_ObsN.txt  \n",
    ">>> image_img.tst.1.png-obs_ObsN.txt  \n",
    ">>>.  \n",
    ">>>.  \n",
    ">>> image_img.tst.N.png-obs_ObsN.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_all_observers(dir_path, img_name):\n",
    "    \"\"\"\n",
    "    List all eye-tracking data files related to a particular image.\n",
    "    \n",
    "    :param dir_path: str, the path to the directory in which the dataset is stored.\n",
    "    :param img_name: str, the name of the image for which we want to gather eye-tracking data.\n",
    "                     This must be of the form img.trn.xxx or img.tst.xxx\n",
    "    :return list_all_files: list, containing the paths to all the relevant eye-tracking files.\n",
    "    \"\"\"\n",
    "    list_all_files = glob.glob(os.path.join(dir_path, 'Eyetracker_Data/**/*' + img_name + '.*'), recursive=True)\n",
    "    assert list_all_files, \"List of eye-tracking files seem to be empty. Check directory or image name.\"\n",
    "    return list_all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96054686",
   "metadata": {},
   "source": [
    "Now that we have our files, we can start having fun with the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18013f64",
   "metadata": {},
   "source": [
    "#### 1) Fill the following function, creating a fixation map based on raw eye-tracking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO ######\n",
    "def create_fixmap(list_obs_files, img_w, img_h, factor_x, factor_y, t_begin=0, t_end=15):\n",
    "    \"\"\"\n",
    "    Create a fixation map based on raw eye-tracking data.\n",
    "    \n",
    "    :param list_obs_files: list, a list of paths to the eye-tracking files of each observer for the considered image\n",
    "    :param img_w: int, the width of the image, in pixels\n",
    "    :param imw_h: int, the height of the image, in pixels\n",
    "    :param factor_x: float, the ratio of the horizontal resolution to the horizontal size of the screen used in the eye-tracking experiment\n",
    "    :param factor_y: float, the ratio of the vertical resolution to the vertical size of the screen used in the eye-tracking experimen\n",
    "    :param t_begin: float, the start of the time slice to consider, in s.\n",
    "    :param t_end: float, the end of the time slice to consider, in s.\n",
    "    :return fixmap: numpy array, the fixation map.\n",
    "    \"\"\"\n",
    "    # First, let's initialize the fixation map.\n",
    "    fixmap = np.zeros((img_h, img_w))\n",
    "    \n",
    "    for obs in list_obs_files:\n",
    "        # We will read the files and store them in a pandas dataframe.\n",
    "        # If you are not familiar with pandas : https://pandas.pydata.org/docs/user_guide/index.html\n",
    "        df = pd.read_csv(obs, sep=',', skiprows=19)\n",
    "        # The 19 first lines of each file are just calibration data. We won't take them into account in this lab.\n",
    "        \n",
    "        # Each observer file represents 15s of viewing.\n",
    "        # However, we might only be interested in a particular time slice of the data, so we'll filter the dataset using\n",
    "        # the input time parameters\n",
    "        \n",
    "        ###### TODO ######\n",
    "        \n",
    "        # Let's also remove all lines where the eye was not tracked (e.g. because of blinks or tracking errors)\n",
    "\n",
    "        ###### TODO ######\n",
    "        \n",
    "        # Now, we want to aggregate all poits belonging to the same fixation into a single point\n",
    "        # For this, we will create groups of consecutive \"Fixation\" values, with a little bit of pandas magic !\n",
    "        fix_groups = df.groupby((df[\"Fixation\"].shift() != df[\"Fixation\"]).cumsum())\n",
    "        for n_group , group in fix_groups:\n",
    "            if group[\"Fixation\"].all() != 0:\n",
    "                # For each detected fixation, we average the location of all gaze points\n",
    "                x = group[\"ScreenPositionXmm\"].mean() * 0.9\n",
    "                y = -group[\"ScreenPositionYmm\"].mean() * 0.9\n",
    "                # Don't worry about the 0.9 coefficient; it's a fix due to errors in the experimental protocol\n",
    "                # The minus sign before the Y coordinate is due to an inversion in the data, that used to be processed\n",
    "                # in Matlab.\n",
    "                # We then find the location of the fixation on the resulting fixation map\n",
    "\n",
    "                ###### TODO ######\n",
    "                \n",
    "                # And we can finally add the fixation to the map\n",
    "                \n",
    "                ###### TODO ######\n",
    "                \n",
    "    return fixmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfbadd",
   "metadata": {},
   "source": [
    "#### 2) Now for a bit of geometry. Create the function compute_ppda, that computes the number of pixels per degree of visual angle based on the experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO ######\n",
    "def compute_ppda(distance, h_res, v_res, screen_w, screen_h):\n",
    "    \"\"\"\n",
    "    Compute the number of pixels per degree of visual angle based on the experimental conditions.\n",
    "    \n",
    "    :param distance: int, the distance between the observer and the screen (in mm)\n",
    "    :param h_res: int, the horizontal resolution of the screen\n",
    "    :param v_res: int, the vertical resolution of the screen\n",
    "    :param screen_w: int, the width of the screen (in mm)\n",
    "    :param screen_h: int, the height of the screen (in mm)\n",
    "    :return horizontal_ppda: float, the number of pixel per degree of visual angle\n",
    "    \"\"\"    \n",
    "    ###### TODO ######\n",
    "    \n",
    "    return horizontal_ppda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775867a5",
   "metadata": {},
   "source": [
    "#### 3) Create a function salmap_from_fixmap generating the visual saliency map based on the fixation map, and the number of pixels per degree of visual angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO ######\n",
    "def salmap_from_fixmap(fixmap, ppda):\n",
    "    \"\"\"\n",
    "    Generate a visual saliency map, based on the fixation map.\n",
    "    \n",
    "    :param fixmap: numpy array, the fixation map\n",
    "    :param ppda: float, the number of pixels per degree of visual angle\n",
    "    :return salmap: numpy array, the visual saliency map\n",
    "    \"\"\"\n",
    "\n",
    "    ###### TODO ######\n",
    "    return salmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7825e17",
   "metadata": {},
   "source": [
    "Now, let's visualize the saliency map we generated !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82258c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_img1 = get_files_all_observers(PATH_DATA, 'img.trn.1')\n",
    "fixmap = create_fixmap(list_img1, 481, 321, FACTOR_X, FACTOR_Y)\n",
    "img = mpimg.imread(os.path.join(PATH_DATA, \"Images\\\\img.trn.1.png\"))\n",
    "# Values for the PPDA computation / image sizes / etc come from the experimental conditions.\n",
    "# You can go read the associated paper for more information !\n",
    "# J. Wang, D. M. Chandler, P. Le Callet, \"Quantifying the relationship between visual salience and visual importance\", Spie Human and Electronic imaging (HVEI) XV, San Jose, 2010\n",
    "\n",
    "ppda = compute_ppda(415.8, 1280, 1024, 338, 270)\n",
    "salmap = salmap_from_fixmap(fixmap, ppda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a72ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fixmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(salmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234fe4e",
   "metadata": {},
   "source": [
    "#### 4) Now it's your turn ! Compute and display the saliency maps for a few images in the dataset. Explore it visually, and try to ensure that you haven't make mistakes while computing it. Vary the parameters to compute the map (assuming that the experimental condition changed); what can you say about it ? How does it influence the final representation, and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c21fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
