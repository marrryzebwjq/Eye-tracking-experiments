{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2ab833",
   "metadata": {},
   "source": [
    "# Lab 2 - Metrics and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9cb8d",
   "metadata": {},
   "source": [
    "Created by : Alexandre Bruckert / University of Nantes - alexandre.bruckert@univ-nantes.fr\n",
    "\n",
    "Date : 2024\n",
    "\n",
    "Inspired by the work of Bylinzkii et. al., *What do different evaluation metrics tell us about saliency models ?*, IEEE PAMI, 41(3), 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a6f65",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to compare visual saliency maps together, and more generally a few keys to quantify visual attention data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b94a",
   "metadata": {},
   "source": [
    "First, we will need a function to draw 2-dimensional Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7702bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(img_size, sigma_filter, locations, values):\n",
    "    img = np.zeros((img_size, img_size))\n",
    "    for loc, val in zip(locations, values):\n",
    "        img[loc[0], loc[1]] = val\n",
    "    img = gaussian_filter(img, sigma=sigma_filter)\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d665d2",
   "metadata": {},
   "source": [
    "Now, we will evaluate the impact of several modifications in dummy saliency maps, to understand the way the metrics work. To do so, we will create two ground truth images : one with a single Gaussian point, and one with two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt1 = create_examples(100, 5, [[25, 25]], [1])\n",
    "gt2 = create_examples(100, 5, [[25, 25], [75, 75]], [1, 1])\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(gt1)\n",
    "axarr[1].imshow(gt2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb49add",
   "metadata": {},
   "source": [
    "With the first ground-truth, we will evaluate the impact of the variance of the prediction, as well as the location of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591b0ae",
   "metadata": {},
   "source": [
    "We will also create a \"fixation ground-truth\", with just a single (or two) fixation point at the center of the cluster(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065374bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_fix_1 = np.zeros((100, 100))\n",
    "gt_fix_1[25,25] = 1\n",
    "gt_fix_2 = np.zeros((100, 100))\n",
    "gt_fix_2[25,25] = 1\n",
    "gt_fix_2[75,75] = 1\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(gt_fix_1)\n",
    "axarr[1].imshow(gt_fix_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70539c30",
   "metadata": {},
   "source": [
    "Now, let's define a few util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_normalize(x):\n",
    "    \"\"\"Normalizes x to [0, 1]\"\"\"\n",
    "    x = (x - x.min()) / (x.max() - x.min())\n",
    "    return x\n",
    "\n",
    "def sum_normalize(x):\n",
    "    \"\"\"Normalizes x so that it sums to 1\"\"\"\n",
    "    return x / x.sum()\n",
    "\n",
    "\n",
    "def std_normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def log_density(saliencyMap, eps=np.spacing(1.0)):\n",
    "    \"\"\"Transforms a non probabilistic saliency map into a log density for\n",
    "    further metric computation.\n",
    "\n",
    "    Arguments:\n",
    "        saliencyMap: Grayscale saliency map.\n",
    "    \"\"\"\n",
    "    saliencyMap = saliencyMap - saliencyMap.min()\n",
    "    saliencyMap += eps\n",
    "    saliencyMap /= saliencyMap.sum()\n",
    "    return np.log(saliencyMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819ffbe",
   "metadata": {},
   "source": [
    "And now, the metrics !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_Borji(saliencyMap, fixationMap, Nsplits=100, stepSize=0.1):\n",
    "    \"\"\"\n",
    "    This measures how well the saliencyMap of an image predicts the ground\n",
    "    truth human fixations on the image.\n",
    "\n",
    "    ROC curve created by sweeping through threshold values at fixed step size\n",
    "    until the maximum saliency map value.\n",
    "    True positive (tp) rate corresponds to the ratio of saliency values\n",
    "    above threshold at fixation locations to the total number of fixation\n",
    "    locations.\n",
    "    False positive (fp) rate corresponds to the ratio of saliency map\n",
    "    values above threshold at random locations to the total number of random\n",
    "    locations (as many random locations as fixations, sampled uniformly from\n",
    "    ALL IMAGE PIXELS), averaging over Nsplits number of selections of\n",
    "    random locations.\n",
    "\n",
    "    :param saliencyMap: numpy array, saliency map\n",
    "    :param fixationMap: numpy array, fixation map (binary matrix)\n",
    "    :param Nsplits: int, number of random splits\n",
    "    :param stepSize: float, size of the step for sweeping through saliency map\n",
    "\n",
    "    :return score: float, AUC Borji score\n",
    "    :return fpr: float, false positive rate\n",
    "    :return tpr: float, true positive rate\n",
    "    \"\"\"\n",
    "    \n",
    "    saliencyMap = range_normalize(saliencyMap)\n",
    "\n",
    "    S = saliencyMap.flatten()\n",
    "    F = fixationMap.flatten()\n",
    "\n",
    "    Sth = S[F > 0]  # saliency map values at fixation locations\n",
    "    Nfixations = len(Sth)\n",
    "    Npixels = len(S)\n",
    "\n",
    "    # For each fixation, sample Nsplits values from anywhere on the saliency map\n",
    "    r = np.random.randint(1, Npixels, size=Nfixations * Nsplits)\n",
    "    randfix = S[r[:]]\n",
    "    randfix = np.reshape(randfix, (Nfixations, Nsplits))\n",
    "\n",
    "    # Calculate AUC per random split (set of random locations)\n",
    "    auc = np.empty(Nsplits)\n",
    "    auc[:] = np.nan\n",
    "    for s in range(Nsplits):\n",
    "        curfix = randfix[:, s]\n",
    "        allthreshes = np.arange(0, np.amax(np.concatenate([Sth, curfix])) + stepSize, stepSize)\n",
    "        allthreshes = allthreshes[::-1]\n",
    "        tpr = np.zeros(len(allthreshes) + 2)\n",
    "        fpr = np.zeros(len(allthreshes) + 2)\n",
    "        tpr[0], tpr[-1] = 0, 1\n",
    "        fpr[0], fpr[-1] = 0, 1\n",
    "        for i in range(len(allthreshes)):\n",
    "            thresh = allthreshes[i]\n",
    "            tpr[i + 1] = (Sth >= thresh).sum() / Nfixations\n",
    "            fpr[i + 1] = (curfix >= thresh).sum() / Nfixations\n",
    "        auc[s] = np.trapz(tpr, x=fpr)\n",
    "    # Average across random splits\n",
    "    score = np.mean(auc)\n",
    "    return score, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_Judd(saliencyMap, fixationMap, jitter=True):\n",
    "    \"\"\"\n",
    "    This measures how well the saliencyMap of an image predicts the ground\n",
    "    truth human fixations on the image. ROC curve created by sweeping through\n",
    "    threshold values determined by range of saliency map values at fixation\n",
    "    locations.\n",
    "    Uses a uniform non-fixation distribution, i.e. the full saliency map as\n",
    "    nonfixations.\n",
    "\n",
    "    created Tilke Judd, Oct 2009\n",
    "    updated  Zoya Bylinskii, Aug 2014\n",
    "    python-version by Dario Zanca, Jan 2017\n",
    "\n",
    "    true positive (tp) rate correspond to the ratio of saliency map values\n",
    "    above threshold at fixation locations to the total number of fixation\n",
    "    locations, false positive (fp) rate correspond to the ratio of\n",
    "    saliency map values above threshold at all other locations to\n",
    "    the total number of posible other locations (non-fixated image pixels)\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        saliencyMap: Saliency map image (grayscale)\n",
    "        fixationMap: Ground truth fixations (binary image)\n",
    "        jitter: Whether to add tiny non-zero random constant to all map\n",
    "            locations to ensure ROC can be calculated robustly\n",
    "            (to avoid uniform region)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        score: float\n",
    "            AUC value for the given inputs\n",
    "    \"\"\"\n",
    "    if not np.shape(saliencyMap) == np.shape(fixationMap):\n",
    "        saliencyMap = cv2.resize(saliencyMap, np.shape(fixationMap)[:2][::-1])\n",
    "\n",
    "    # jitter saliency maps that come from saliency models that have a lot of\n",
    "    # zero values.\n",
    "    # If the saliency map is made with a Gaussian then it does not need to be\n",
    "    # jittered as the values are varied and there is not a large patch of the\n",
    "    # same value. In fact jittering breaks the ordering in the small values!\n",
    "    if jitter:\n",
    "        # jitter the saliency map slightly to disrupt ties of the same numbers\n",
    "        saliencyMap = saliencyMap + np.random.random(np.shape(saliencyMap)) / 10**7\n",
    "\n",
    "    saliencyMap = range_normalize(saliencyMap)\n",
    "\n",
    "    if np.isnan(saliencyMap).all():\n",
    "        __logger.debug('NaN saliencyMap')\n",
    "        score = float('nan')\n",
    "        return score\n",
    "\n",
    "    S = saliencyMap.flatten()\n",
    "    F = fixationMap.flatten()\n",
    "\n",
    "    # Sal map values at fixation locations\n",
    "    Sth = S[F > 0]\n",
    "    Nfixations = len(Sth)\n",
    "    Npixels = len(S)\n",
    "\n",
    "    # sort sal map values, to sweep through values\n",
    "    allthreshes = sorted(Sth, reverse=True)\n",
    "\n",
    "    tpr = np.zeros((Nfixations + 2))\n",
    "    fpr = np.zeros((Nfixations + 2))\n",
    "    tpr[0], tpr[-1] = 0, 1\n",
    "    fpr[0], fpr[-1] = 0, 1\n",
    "\n",
    "    for i in range(Nfixations):\n",
    "        thresh = allthreshes[i]\n",
    "        # total number of sal map values above threshold\n",
    "        aboveth = (S >= thresh).sum()\n",
    "        # ratio sal map values at fixation locations above threshold\n",
    "        tpr[i + 1] = float(i + 1) / Nfixations\n",
    "        # ratio other sal map values above threshold\n",
    "        fpr[i + 1] = float(aboveth - i) / (Npixels - Nfixations)\n",
    "\n",
    "    score = np.trapz(tpr, x=fpr)\n",
    "    allthreshes = np.insert(allthreshes, 0, 0)\n",
    "    allthreshes = np.append(allthreshes, 1)\n",
    "\n",
    "    return score, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(saliencyMap, baselineMap, eps=np.spacing(1.0)):\n",
    "    \"\"\"\n",
    "    Computes the _Image Based_ KL-divergence between two different saliency\n",
    "    maps when viewed as distributions: it is a non-symmetric measure\n",
    "    of the information lost when saliencyMap is used to estimate a 'true'\n",
    "    distribution.\n",
    "\n",
    "    created: Zoya Bylinskii, Aug 2014\\\\\n",
    "    python-version by: Dario Zanca/Pierre-Adrien Fons, 2017-20\n",
    "\n",
    "    Arguments:\n",
    "        saliencyMap: Grayscale image of a saliency map.\n",
    "        baselineMap: Grayscale image of a baseline saliency map to compare\n",
    "            saliencyMap to.\n",
    "\n",
    "    Returns:\n",
    "        KL divergence score (Non negative).\n",
    "    \"\"\"\n",
    "\n",
    "    if saliencyMap.shape != baselineMap.shape:\n",
    "        saliencyMap = cv2.resize(saliencyMap, np.shape(baselineMap)[:2][::-1])\n",
    "\n",
    "    if saliencyMap.any():\n",
    "        saliencyMap = sum_normalize(saliencyMap)\n",
    "    if baselineMap.any():\n",
    "        baselineMap = sum_normalize(baselineMap)\n",
    "\n",
    "    logp_model = np.log(saliencyMap + eps)\n",
    "    logp_gt = np.log(baselineMap + eps)\n",
    "    score = baselineMap * (logp_gt - logp_model)\n",
    "\n",
    "    return score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NSS(saliencyMap, fixationMap):\n",
    "    \"\"\"\n",
    "    This finds the normalized scanpath saliency (NSS) of a saliency map.\\\\\n",
    "    NSS is the average of the response values at human eye positions in a model\n",
    "    saliency map that has been normalized to have zero mean and unit standard\n",
    "    deviation.\n",
    "\n",
    "    created: Zoya Bylinskii, Aug 2014\\\\\n",
    "    python-version by: Dario Zanca/Pierre-Adrien Fons, 2017-20\n",
    "\n",
    "    Arguments:\n",
    "        saliencyMap: Saliency map (grayscale).\n",
    "        fixationMap: Ground truth fixation map (binary matrix).\n",
    "\n",
    "    Returns:\n",
    "        NSS score (0 : Chance, >0 : correspondance above Chance, <0 : anti\n",
    "            correspondance).\n",
    "    \"\"\"\n",
    "\n",
    "    if saliencyMap.shape != np.shape(fixationMap):\n",
    "        saliencyMap = cv2.resize(saliencyMap, np.shape(fixationMap)[:2][::-1])\n",
    "\n",
    "    saliencyMap = np.exp(log_density(saliencyMap))\n",
    "\n",
    "    mean = saliencyMap.mean()\n",
    "    std = saliencyMap.std()\n",
    "\n",
    "    value = saliencyMap[fixationMap.astype(bool)]\n",
    "\n",
    "    value -= mean\n",
    "    value /= std\n",
    "\n",
    "    return value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CC(saliencyMap1, saliencyMap2):\n",
    "    \"\"\"\n",
    "    Computes the linear correlation coefficient between two different\n",
    "    saliency maps (also called Pearson's linear coefficient).\n",
    "\n",
    "    Arguments:\n",
    "        saliencyMap1: Grayscale Saliency map.\n",
    "        saliencyMap2: Grayscale Saliency map.\n",
    "\n",
    "    Returns:\n",
    "        Linear correlation coefficient ([-1, 1]).\n",
    "    \"\"\"\n",
    "\n",
    "    if saliencyMap1.shape != saliencyMap2.shape:\n",
    "        saliencyMap1 = cv2.resize(saliencyMap1, np.shape(saliencyMap2)[:2][::-1])\n",
    "\n",
    "    saliencyMap1 = std_normalize(saliencyMap1)\n",
    "    saliencyMap2 = std_normalize(saliencyMap2)\n",
    "\n",
    "    return np.corrcoef(saliencyMap1.reshape(-1), saliencyMap2.reshape(-1))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(pred_sal, gt_sal):\n",
    "    \"\"\"\n",
    "    This finds the similarity between two different saliency maps when\n",
    "    viewed as distributions (equivalent to histogram intersection).\n",
    "\n",
    "    score=1 means the maps are identical\n",
    "    score=0 means the maps are completely opposite\n",
    "\n",
    "    \"SIM is very sensitive to missing values, and penalizes predictions that\n",
    "    fail to account for all of the ground truth density.\"\n",
    "\n",
    "    Arguments:\n",
    "        pred_sal: Predicted saliency map (grayscale).\n",
    "        gt_sal: Ground truth saliency map (grayscale).\n",
    "\n",
    "    Returns:\n",
    "        Score of similarity [0, 1] between the two input saliency maps.\n",
    "    \"\"\"\n",
    "    if pred_sal.shape != gt_sal.shape:\n",
    "        pred_sal = cv2.resize(pred_sal, gt_sal.shape[:2][::-1])\n",
    "\n",
    "    # (1) first normalize the map values to lie between 0-1 this is done so\n",
    "    # that models that assign a nonzero value to every pixel do not get an\n",
    "    # artificial performance boost.\n",
    "    # (2) then make sure that the map is normalized to sum to 1 so that the\n",
    "    # maximum value of score will be 1.\n",
    "    if pred_sal.any():\n",
    "        pred_sal = range_normalize(pred_sal)\n",
    "        pred_sal = sum_normalize(pred_sal)\n",
    "\n",
    "    if gt_sal.any():\n",
    "        gt_sal = range_normalize(gt_sal)\n",
    "        gt_sal = sum_normalize(gt_sal)\n",
    "\n",
    "    diff = np.minimum(pred_sal, gt_sal)\n",
    "    return np.sum(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33ba48",
   "metadata": {},
   "source": [
    "#### 1) Create \"predictions\" by changing the variance parameter, and evaluate these predictions against the ground-truth. For each metric, draw a curve showing the evolution of the measure depending on the variance.\n",
    "\n",
    "#### 2) Do the same by moving the location of the point on the main diagonal. What can you say about the behaviour of each metric ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9da30",
   "metadata": {},
   "source": [
    "With the second ground-truth, we will observe what happens when a prediction moves between two correct attention clusters, and what happens when the relative weights between the two clusters vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5472dc",
   "metadata": {},
   "source": [
    "#### 3) Similarly, create \"predictions\" with only one point moving on the main diagonal, between the first and second cluster. Plot the evolution curve for each metric.\n",
    "\n",
    "#### 4) Do the same by moving the relative weights of the two clusters, and comment your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
